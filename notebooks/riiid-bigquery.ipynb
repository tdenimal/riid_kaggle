{"cells":[{"metadata":{"id":"8QCO0cr_juQA"},"cell_type":"markdown","source":["## Intro"]},{"metadata":{"id":"L1mQ0DiRnfer"},"cell_type":"markdown","source":["This kernel is an end to end pipeline that uses BigQuery to store data and perform feature engineering, and trains a model using XGBoost. I was resorting to breaking up tables and still waiting a long time to see the results of my analysis and to process my engineered features, so I decided to learn about BigQuery. This kernel is the current state of my setup, which is working very well. It is much faster than my previous local setup, even with having to download files. It also is making it easier to keep the structure of the data and and code clean, which in turn makes it easier to stay focused on thinking about and executing ideas without getting bogged down waiting for things to finish or wading through extraneous processing code.\n","\n","I've attempted to put  this book together in such a way that somebody else can fork it, update a few environment variables, run it and then be in the game engineering features and improving the model. The only requirements are a GCP project and storage bucket. Other than that, it is turn key, starting with creating a BigQuery dataset and ending with a saved model and two feature tables that get uploaded to a Kaggle dataset where they are used in a separate kernel to make predictions and submit to the competition api.\n","\n","A couple of cool features:\n","* Uses the gcs version of the competition datset to create a dataset and upload to BigQuery in around a minute\n","* Transformations get run on the entire train table at once and run in under 10 minutes\n","* Feature engineering gets done on a sample of the train table, taking advantage of BigQuery' graphical query editing interface that includes tab completion, syntax checking and the ability to run queries and inspect results\n","* Stores queries as methods on a dedicated class, where they can be easily reused\n","* Dtypes for local dataframes, schema for BigQuery tables and all tranformations are maintained locally so that the transformed tables can be recreated from the original competition dataset files automatically at any time (see description of workflow below to continue with this practice)\n","* Exports to gcs using temporary tables created by BigQuery avoiding unnecessary storage and wasted time rerunning and exporting duplicate queries\n","* Separate [submission kernel](https://www.kaggle.com/calebeverett/riiid-submit) uses sqlite3 to achieve sub two hour submission times while maintaining state for questions, users and user-content (80+ million rows)\n","\n","I've engineered a few features as a starting point to demonstrate how additional features can be efficiently developed and processed, including:\n","* Cumulative and rolling sums of questions answered correctly and incorrectly by user\n","* Percent of questions answered correctly by question id, part and the first question tag\n","\n","The model is also just a starting point, with a first pass at a train/validation split and no hyperparameter tuning. I have included some basic diagnostics on both the train/validtion split and model performance as a starting place for further development.\n","\n","I have the table creation and transformation functions set to not run, but you can set them to run, by changing the flags to `True` for:\n","* Loading tables - one flag for the questions table and another for the train and lectures tables\n","* Updating the schemas in BigQuery\n","* Performing the transformations"]},{"metadata":{"id":"savju5R9juQB"},"cell_type":"markdown","source":["## Resources\n","* [BigQuery Console](https://console.cloud.google.com/bigquery)\n","* [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html)\n","* [Analytic function concepts in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/analytic-function-concepts)\n","* [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/index.html)\n","* [Storge Client](https://googleapis.dev/python/storage/latest/client.html)\n","* [pandas documentation](https://pandas.pydata.org/docs/)\n","* [Plotly Python Open Source Graphing Library](https://plotly.com/python/)\n","* [PEP 8 -- Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/)"]},{"metadata":{"id":"88t93RZzj982"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"tKe4S7M4nbSU","_kg_hide-input":true,"trusted":true,"tags":[]},"cell_type":"code","source":["# <hide-input>\n","%load_ext autoreload\n","%autoreload 2\n","\n","from datetime import datetime\n","import gc\n","import json\n","import os\n","from pathlib import Path\n","import re\n","import subprocess\n","import sys\n","import time\n","\n","import ipywidgets as widgets\n","from google.cloud import storage, bigquery\n","from google.cloud.bigquery import SchemaField\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","BUCKET = 'kaggle_tdenimal'\n","DATASET = 'riid'\n","LOCATION = 'US'\n","KAGGLE_SUBMIT_DATASET = 'riiid-submission'\n","PROJECT = 'learninggcp82'\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../data/01_raw/learninggcp82-9d40293c9b3b.json\"\n","#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../input/gcp-credentials/learninggcp82-9d40293c9b3b.json\"\n","    \n","bucket = storage.Client(project=PROJECT).get_bucket(BUCKET)\n","dataset = bigquery.Dataset(f'{PROJECT}.{DATASET}')\n","bq_client = bigquery.Client(project=PROJECT, location=LOCATION)\n","\n","import plotly\n","import plotly.express as px\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import MultiLabelBinarizer\n","# import xgboost as xgb\n","pd.options.plotting.backend = 'plotly'"],"execution_count":1,"outputs":[]},{"metadata":{"id":"3I6-yy4QBxlR"},"cell_type":"markdown","source":["## Modules\n","Included in notebook for convenience when in a Kaggle kernel. Github repo [here](https://github.com/CalebEverett/riiid_2020)."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"'learninggcp82'"},"metadata":{},"execution_count":45}],"source":["self.bq_client.project"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":56,"metadata":{"_kg_hide-input":true,"id":"--uRkD6w0EhS","trusted":true},"outputs":[],"source":["from pathlib import Path\n","import pytz\n","import sys\n","import os\n","\n","from google.cloud.bigquery import ExtractJobConfig, LoadJobConfig, \\\n","    SchemaField, SourceFormat\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","\n","class BQHelper:\n","    def __init__(self, bucket, DATASET, bq_client):\n","        self.bucket = bucket\n","        self.BUCKET = self.bucket.name\n","        self.DATASET = DATASET\n","        self.bq_client = bq_client\n","   \n","    # LOAD FUNCTTIONS \n","    # ================\n","    def upload_file_gcs(self,filepath):\n","        \"\"\"Upload files to GCP bucket.\"\"\"\n","        blob = self.bucket.blob(os.path.basename(filepath))\n","        blob.upload_from_filename(filepath)\n","        return f'Uploaded {filepath} to \"{self.bucket.name}\" bucket.'\n","    \n","    \n","    \n","    def load_job_cb(self, future):\n","        \"\"\"Prints update upon completion to output of last run cell.\"\"\"\n","        \n","        seconds = (future.ended - future.created).total_seconds()\n","        print(f'Loaded {future.output_rows:,d} rows to table {future.job_id.split(\"_\")[0]} in '\n","            f'{seconds:>4,.1f} sec, {int(future.output_rows / seconds):,d} per sec.')\n","        \n","    def load_csv_uri(self, table_id):\n","        full_table_id = f'{self.DATASET}.{table_id}'\n","\n","        job_config = LoadJobConfig(\n","            autodetect=True,\n","            source_format=SourceFormat.CSV,\n","            skip_leading_rows=1\n","            )\n","\n","        uri = f'gs://{self.BUCKET}/{table_id}.csv'\n","        load_job = self.bq_client.load_table_from_uri(uri, full_table_id,\n","                                                job_config=job_config,\n","                                                job_id_prefix=f'{table_id}_')\n","        print(f'job {load_job.job_id} started')\n","        load_job.add_done_callback(self.load_job_cb)\n","        \n","        return load_job\n","        \n","    def load_json_file(self, table_id, schema):\n","        full_table_id = f'{self.DATASET}.{table_id}'\n","\n","        job_config = LoadJobConfig(\n","            schema=schema,\n","            source_format=SourceFormat.NEWLINE_DELIMITED_JSON)\n","\n","        file_path = f'{table_id}.json'\n","        with open(file_path, \"rb\") as source_file:\n","            load_job = self.bq_client.load_table_from_file(source_file,\n","                                                    full_table_id,\n","                                                    job_config=job_config,\n","                                                    job_id_prefix=f'{table_id}_')\n","        print(f'job {load_job.job_id} started')\n","        load_job.add_done_callback(self.load_job_cb)\n","        \n","        return load_job\n","\n","    def get_table(self, table_id):\n","        return self.bq_client.get_table(f'{self.DATASET}.{table_id}')\n","\n","    def del_table(self, table_id):\n","        return self.bq_client.delete_table(f'{self.DATASET}.{table_id}',\n","                                    not_found_ok=True)\n","\n","    def get_df_jobs(self, max_results=10):\n","        jobs = self.bq_client.list_jobs(max_results=max_results, all_users=True)\n","        jobs_list = []\n","\n","        if jobs.num_results:\n","            for job in jobs:\n","                ended = job.ended if job.ended else datetime.now(pytz.UTC)\n","                exception = job.exception() if job.ended else None\n","                jobs_list.append({'job_id': job.job_id, 'job_type': job.job_type,\n","                            'started': job.started, 'ended': ended,\n","                            'running': job.running(),\n","                            'exception': exception,\n","                            })\n","            df_jobs = pd.DataFrame(jobs_list)\n","            df_jobs['seconds'] = (df_jobs.ended - df_jobs.started).dt.seconds\n","            df_jobs.started = df_jobs.started.astype(str).str[:16]\n","            del df_jobs['ended']\n","            return df_jobs\n","        else:\n","            return None\n","\n","    def get_df_table_list(self):\n","        tables = []\n","        for t in self.bq_client.list_tables(self.DATASET):\n","            table = self.bq_client.get_table(t)\n","            tables.append({'table_id': table.table_id, 'cols': len(table.schema),\n","                        'rows': table.num_rows, 'kb': int(table.num_bytes/1e3)})\n","        df_tables = pd.DataFrame(tables)\n","        \n","        return df_tables\n","\n","    # QUERY FUNCTIONS\n","    # ================\n","    def done_cb(self, future):\n","        seconds = (future.ended - future.started).total_seconds()\n","        print(f'Job {future.job_id} finished in {seconds} seconds.')\n","\n","    def run_query(self, query, job_id_prefix=None, wait=False):\n","        query_job = self.bq_client.query(query, job_id_prefix=job_id_prefix)\n","        print(f'Job {query_job.job_id} started.')\n","        query_job.add_done_callback(self.done_cb)\n","        if wait:\n","            query_job.result()\n","        \n","        return query_job\n","\n","    def get_df_query(self, query, dtypes=None):\n","        query_job = self.run_query(*query)\n","\n","        df_query = query_job.to_dataframe(dtypes=dtypes, \n","                                progress_bar_type='tqdm_notebook')\n","        return df_query\n","\n","    def get_df_table(self, table_id, max_results=10000, dtypes=None):\n","        table = self.get_table(table_id)\n","        df_table = (self.bq_client.list_rows(table, max_results=max_results)\n","                    .to_dataframe(dtypes=dtypes,\n","                                progress_bar_type='tqdm_notebook'))\n","        return df_table\n","\n","    # EXPORT FUNCTIONS\n","    # ================\n","    def export_query_gcs(self, query, file_format='csv', wait=True):\n","        \"\"\" Uses BigQuery temporary table reference as gcs prefix.\n","        Runs query and exports to gcs if it doesn't already exist in gcs.\n","        Exported in multiple files if over 1GB. Returns gcs prefix.\n","        \"\"\"\n","        qj = self.run_query(*query, wait=wait)\n","        \n","        prefix = ('/').join(qj.destination.path.split('/')[-2:])\n","        blobs_list = list(self.bucket.list_blobs(prefix=prefix))\n","        \n","        if not blobs_list:\n","            \n","            job_prefix_id = sys._getframe().f_code.co_name + '_'\n","            \n","            formats={'csv': 'CSV',\n","                    'json': 'NEWLINE_DELIMITED_JSON'}\n","            \n","            job_config = ExtractJobConfig(destination_format=formats[file_format])\n","            \n","            ex_job = self.bq_client.extract_table(\n","                source=qj.destination,\n","                destination_uris=f'gs://{self.BUCKET}/{prefix}/*.{file_format}',\n","                job_id_prefix=job_prefix_id,\n","                job_config=job_config)\n","        \n","            ex_job.add_done_callback(self.done_cb)\n","            \n","            print(f'Job {ex_job.job_id} started.')\n","        \n","            if wait:\n","                ex_job.result()\n","                blobs_list = list(self.bucket.list_blobs(prefix=prefix))\n","                n_files = len(blobs_list) \n","                print(f'{n_files} file{\"s\" if n_files > 1 else \"\"} '\n","                    f'exported to gcs with prefix {prefix}.')\n","        \n","        else:\n","            n_files = len(blobs_list) \n","            print(f'{n_files} file{\"s\" if n_files > 1 else \"\"} '\n","                  f'already exist{\"s\" if n_files == 1 else \"\"} in gcs with '\n","                  f'prefix {prefix}.')\n","        \n","        return prefix\n","\n","    def get_table_gcs(self, prefix):\n","        \"\"\"Downloads all files at prefix if they don't exist locally.\n","        Returns list of file paths.\n","        \"\"\"\n","        \n","        file_paths = list(Path().glob(prefix))\n","        if not file_paths:\n","            blobs_list = list(self.bucket.list_blobs(prefix=prefix))\n","            n_files = len(blobs_list)\n","            print(f'Downloading {n_files} file{\"s\" if n_files > 1 else \"\"} '\n","                  f'from gcs for table {prefix}...')\n","\n","            for b in tqdm(blobs_list, desc='Files Downloaded: '):\n","                print('Downloading', b.name, b.size)\n","                Path(b.name).parent.mkdir(parents=True, exist_ok=True)\n","                b.download_to_filename(b.name)\n","        \n","        else:\n","            n_files = len(list(file_paths[0].iterdir()))\n","            print(f'{n_files} file{\"s\" if n_files > 1 else \"\"} already '\n","                  f'exist{\"s\" if n_files == 1 else \"\"} locally for '\n","                  f'table{prefix}.')\n","            \n","        return list(list(Path().glob(prefix))[0].iterdir())\n","\n","    def get_df_files(self, file_paths, dtypes):\n","        \"\"\" Creates data frame from list of local file paths.\n","        Returns dataframe.\n","        \"\"\"\n","        \n","        prefix = str(file_paths[0].parent)\n","        suffix = file_paths[0].suffix\n","        \n","        n_files = len(file_paths)\n","        print(f'Creating dataframe from {n_files} file{\"s\" if n_files > 1 else \"\"} for table {prefix}...')\n","        \n","        dfs = []\n","        if suffix == '.csv':\n","            for f in tqdm(file_paths, desc='Files Read: '):\n","                dfs.append(pd.read_csv(f, dtype=dtypes))\n","        else:\n","            for f in tqdm(file_paths, desc='Files Read: '):\n","                dfs.append(pd.read_json(f, dtype=dtypes, lines=True))\n","        \n","        df_train = pd.concat(dfs)\n","        \n","        print(f'Dataframe finished for train table at {prefix} with'\n","            f' {len(df_train.columns):,d} columns and '\n","            f'{len(df_train):,d} rows.')\n","        \n","        return df_train\n","\n","    def get_df_query_gcs(self, query, dtypes, file_format='csv', wait=True):\n","        prefix = self.export_query_gcs(query, file_format, wait)\n","        file_paths = self.get_table_gcs(prefix)\n","        df = self.get_df_files(file_paths, dtypes)\n","        \n","        return df# <include-bqhelpers.py><hide-input>\n","\n","    \n","    def update_table_schema(self,table_name,new_fields):\n","        # table_id = \"your-project.your_dataset.your_table_name\"\n","        table_id = self.bq_client.project+\".\"+self.DATASET+\".\"+table_name\n","        table = self.bq_client.get_table(table_id)  # Make an API request.\n","\n","        original_schema = table.schema\n","        new_schema = original_schema[:]  # Creates a copy of the schema.\n","\n","        #Append new field to schema\n","        for field in new_fields:\n","            new_schema.append(field)\n","\n","        table.schema = new_schema\n","        table = self.bq_client.update_table(table, [\"schema\"])  # Make an API request."]},{"metadata":{"id":"dTP7zTkV0qhK","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["import sys\n","\n","class Queries:\n","    def __init__(self, DATASET):\n","        self.DATASET = DATASET\n","    \n","    def select_rows(self, table_id='train', limit=100):\n","        return f\"\"\"\n","            SELECT *\n","            FROM {self.DATASET}.{table_id}\n","            LIMIT {limit}\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def update_missing_values(self, table_id='train', column_id=None, value=None):\n","        return f\"\"\"\n","            UPDATE {self.DATASET}.{table_id}\n","            SET {column_id} = {value}\n","            WHERE {column_id} is NULL;\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def update_task_container_id(self, table_id='train'):\n","        return f\"\"\"\n","            UPDATE {self.DATASET}.{table_id} t\n","            SET task_container_id = target.calc\n","            FROM (\n","              SELECT row_id, DENSE_RANK()\n","                OVER (\n","                  PARTITION BY user_id\n","                  ORDER BY timestamp\n","                ) - 1 calc\n","              FROM {self.DATASET}.{table_id}\n","            ) target\n","            WHERE target.row_id = t.row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def create_train_sample(self, suffix='sample', user_id_max=50000):\n","        return f\"\"\"\n","            CREATE OR REPLACE TABLE {self.DATASET}.train_{suffix} AS\n","            SELECT *\n","            FROM {self.DATASET}.train\n","            WHERE user_id <= {user_id_max}\n","            ORDER BY user_id, task_container_id, row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def select_train(self, columns=['*'], user_id_max=50000,\n","                     excl_lectures=False, table_id='train'):\n","        \n","        where_condition = f'user_id <= {user_id_max}' if user_id_max else 'true'\n","        where_condition = (where_condition + ' AND content_type_id = 0'\n","                           if excl_lectures else where_condition)\n","        \n","        return f\"\"\"\n","            SELECT {(', ').join(columns)}\n","            FROM {self.DATASET}.{table_id} t\n","            LEFT JOIN {self.DATASET}.questions q\n","            ON t.content_id = q.question_id\n","            WHERE {where_condition}\n","            ORDER BY user_id, task_container_id, row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","    \n","    def update_answered_incorrectly(self, table_id='train'):\n","        \"\"\"Sets annswered_incorrectly to inverse of answered_correctly for questions.\n","        Sets answered_correctly to 0 for lectures so window totals for correct and\n","        incorrect are caculated correctly, including lectures.\n","        \"\"\"\n","\n","\n","\n","        \n","    \n","        return f\"\"\"\n","            UPDATE {self.DATASET}.{table_id}\n","            SET answered_incorrectly = 0\n","            WHERE true;\n","\n","            UPDATE {self.DATASET}.{table_id}\n","            SET answered_incorrectly = 1 - answered_correctly\n","            WHERE content_type_id = 0;\n","\n","            UPDATE {self.DATASET}.{table_id}\n","            SET answered_correctly = 0\n","            WHERE content_type_id = 1;\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","\n","    def update_questions_tag__0(self):\n","        return f\"\"\"\n","            UPDATE data.questions\n","            SET tag__0 = tags[OFFSET(0)]\n","            WHERE true;\n","        \"\"\", sys._getframe().f_code.co_name + '_'    \n","    \n","    def update_train_window_containers(self, table_id='train'):\n","        return f\"\"\"            \n","        UPDATE {self.DATASET}.{table_id} t\n","        SET answered_correctly_cumsum = IFNULL(calc.answered_correctly_cumsum, 0),\n","            answered_incorrectly_cumsum = IFNULL(calc.answered_incorrectly_cumsum, 0),\n","            lectures_cumcount = IFNULL(calc.lectures_cumcount, 0),\n","            prior_question_elapsed_time_rollavg = IFNULL(calc.prior_question_elapsed_time_rollavg, 0),\n","            answered_correctly_content_id_cumsum = IFNULL(calc.answered_correctly_content_id_cumsum, 0),\n","            answered_incorrectly_content_id_cumsum = IFNULL(calc.answered_incorrectly_content_id_cumsum, 0)\n","        FROM (\n","        SELECT row_id,\n","            SUM(answered_correctly) OVER (b) answered_correctly_cumsum,\n","            SUM(answered_incorrectly) OVER (b) answered_incorrectly_cumsum,\n","            SUM(content_type_id) OVER (b) lectures_cumcount,\n","            AVG(prior_question_elapsed_time) OVER (c) prior_question_elapsed_time_rollavg,\n","            SUM(answered_correctly) OVER (e) answered_correctly_content_id_cumsum,\n","            SUM(answered_incorrectly) OVER (e) answered_incorrectly_content_id_cumsum\n","        FROM {self.DATASET}.{table_id}\n","        WINDOW\n","            a AS (PARTITION BY user_id ORDER BY task_container_id),\n","            b AS (a RANGE BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING),\n","            c AS (a RANGE BETWEEN 3 PRECEDING AND 0 PRECEDING),\n","            d AS (PARTITION BY user_id, content_id ORDER BY task_container_id),\n","            e AS (d RANGE BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\n","        ORDER BY user_id, task_container_id, row_id\n","        ) calc\n","        WHERE calc.row_id = t.row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def update_train_window_rows(self, table_id='train', window=10):\n","        \"\"\"Calculates aggregate over window number of rows with task_container_id\n","        less than task_container_id of current row.\n","        \"\"\"\n","\n","        return f\"\"\"            \n","        UPDATE {self.DATASET}.{table_id} u\n","        SET answered_correctly_rollsum = IFNULL(calc.answered_correctly_rollsum, 0),\n","            answered_incorrectly_rollsum = IFNULL(calc.answered_incorrectly_rollsum, 0)\n","        FROM (\n","        SELECT t.row_id,\n","            COUNT(j2.row_id) row_id_rollcount,\n","            SUM(j2.answered_correctly) answered_correctly_rollsum,\n","            SUM(j2.answered_incorrectly) answered_incorrectly_rollsum,\n","        FROM {self.DATASET}.{table_id} t\n","        JOIN (\n","            SELECT user_id, task_container_id, MIN(row_id) min_row\n","            FROM {self.DATASET}.{table_id}\n","            GROUP BY user_id, task_container_id\n","        ) j ON (j.user_id = t.user_id AND j.task_container_id = t.task_container_id)\n","        LEFT JOIN {self.DATASET}.{table_id} j2 ON (\n","            j2.user_id = t.user_id\n","            AND j2.task_container_id < t.task_container_id\n","            AND j2.row_id >= (j.min_row - {window + 1})\n","        )\n","        GROUP BY t.user_id, t.task_container_id, t.row_id\n","        ) calc\n","        WHERE\n","        calc.row_id = u.row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","\n","    def update_answered_correctly_cumsum_upto(self, table_id='train'):        \n","        return f\"\"\"            \n","        UPDATE {self.DATASET}.{table_id} t\n","        SET answered_correctly_cumsum_upto = IF(row_number < 11, r.answered_correctly_cumsum, m.ac_max)\n","        FROM (\n","        SELECT user_id, row_id, answered_correctly_cumsum,\n","            ROW_NUMBER() OVER(W) row_number,\n","        FROM {self.DATASET}.{table_id}\n","        WHERE content_type_id = 0\n","        WINDOW\n","            w AS (PARTITION BY user_id ORDER BY row_id)\n","        ) r\n","        JOIN (\n","        SELECT user_id, MAX(answered_correctly_cumsum) ac_max\n","        FROM (\n","            SELECT user_id, row_id, answered_correctly_cumsum,\n","            ROW_NUMBER() OVER(W) row_number,\n","            FROM {self.DATASET}.{table_id}\n","            WINDOW\n","                w AS (PARTITION BY user_id ORDER BY row_id)\n","        )\n","        WHERE row_number < 11\n","        GROUP BY user_id\n","        ) m\n","        ON (m.user_id = r.user_id)\n","        WHERE r.row_id = t.row_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def update_correct_cumsum_pct(self, column_id_correct=None,\n","                                  column_id_incorrect=None,\n","                                  update_column_id=None, table_id='train'):\n","        return f\"\"\"\n","            CREATE TEMP FUNCTION calcCorrectPct(c INT64, ic INT64) AS (\n","              CAST(SAFE_DIVIDE(c, (c + ic)) * 100 AS INT64)\n","            );\n","\n","            UPDATE {self.DATASET}.{table_id}\n","            SET {update_column_id} =\n","                calcCorrectPct({column_id_correct}, {column_id_incorrect})\n","            WHERE true;\n","            \n","            UPDATE {self.DATASET}.{table_id}\n","            SET {update_column_id} = 0\n","            WHERE {update_column_id} IS NULL;\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def update_question_correct_pct(self, column_id):\n","        return f\"\"\"  \n","            CREATE TEMP FUNCTION calcCorrectPct(c INT64, ic INT64) AS (\n","              CAST(SAFE_DIVIDE(c, (c + ic)) * 100 AS INT64)\n","            );\n","\n","            UPDATE {self.DATASET}.questions q\n","            SET q.{column_id}_correct_pct = calcCorrectPct(c.c, c.ic)\n","            FROM (\n","                SELECT cq.{column_id}, SUM(answered_correctly) c, SUM(answered_incorrectly) ic\n","                FROM {self.DATASET}.train t\n","                JOIN {self.DATASET}.questions cq\n","                ON t.content_id = cq.question_id\n","                WHERE t.content_type_id = 0\n","                GROUP BY cq.{column_id}\n","            ) c\n","            WHERE q.{column_id} = c.{column_id}\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def select_user_id_rows(self, table_id='train', rows=30000):\n","        return f\"\"\"            \n","            SELECT user_id\n","            FROM {self.DATASET}.{table_id}\n","            WHERE row_id = {rows}\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","    \n","    def select_user_final_state(self, table_id='train'):\n","        return f\"\"\"            \n","        CREATE TEMP FUNCTION calcCorrectPct(c INT64, ic INT64) AS (\n","        IFNULL(CAST(SAFE_DIVIDE(c, (c + ic)) * 100 AS INT64), 0)\n","        );\n","        \n","        SELECT *, calcCorrectPct(answered_correctly_cumsum, answered_incorrectly_cumsum) answered_correctly_cumsum_pct,\n","        calcCorrectPct(answered_correctly_rollsum, answered_incorrectly_rollsum) answered_correctly_rollsum_pct\n","        FROM (\n","        SELECT row_id, user_id, answered_correctly_cumsum_upto, content_type_id,\n","            SUM(answered_correctly) OVER (b) answered_correctly_cumsum,\n","            SUM(answered_incorrectly) OVER (b) answered_incorrectly_cumsum,\n","            SUM(answered_correctly) OVER (d) answered_correctly_rollsum,\n","            SUM(answered_incorrectly) OVER (d) answered_incorrectly_rollsum,\n","            SUM(content_type_id) OVER (b) lectures_cumcount,\n","            AVG(prior_question_elapsed_time) OVER (c) prior_question_elapsed_time_rollavg,\n","            ROW_NUMBER() OVER(y) row_no_desc,\n","            SUM(answered_correctly + answered_incorrectly) OVER (d) answer_row_id_rollcount,\n","            SUM(answered_correctly + answered_incorrectly) OVER (c) time_row_id_rollcount,\n","            SUM(answered_correctly + answered_incorrectly) OVER (a) question_row_id_rollcount,\n","        FROM {self.DATASET}.{table_id}\n","        WINDOW\n","            x AS (PARTITION BY user_id),\n","            y AS (x ORDER BY task_container_id DESC, row_id DESC),\n","            a AS (x ORDER BY task_container_id),\n","            b AS (a ROWS BETWEEN UNBOUNDED PRECEDING AND 0 PRECEDING),\n","            c AS (a RANGE BETWEEN 3 PRECEDING AND 0 PRECEDING),\n","            d AS (a ROWS BETWEEN 9 PRECEDING AND 0 PRECEDING)\n","        )\n","        WHERE row_no_desc = 1 AND content_type_id = 0\n","        ORDER BY user_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","\n","    def select_user_content_final_state(self, table_id='train'):\n","        return f\"\"\"            \n","        SELECT user_id, content_id, SUM(answered_correctly) answered_correctly,\n","            SUM(answered_incorrectly) answered_incorrectly,\n","        FROM {self.DATASET}.{table_id}\n","        WHERE content_type_id = 0\n","        GROUP BY user_id, content_id\n","        ORDER BY user_id, content_id\n","        \"\"\", sys._getframe().f_code.co_name + '_'\n","    # <include-queries.py><hide-input>"],"execution_count":35,"outputs":[]},{"metadata":{"id":"WO6DVIxuQydR","trusted":true},"cell_type":"code","source":["Q = Queries(DATASET)\n","bqh = BQHelper(bucket, DATASET, bq_client)"],"execution_count":57,"outputs":[]},{"metadata":{"id":"cs6bXr35juQM"},"cell_type":"markdown","source":["## Create BigQuery Dataset"]},{"metadata":{"trusted":true,"id":"MVloHU3wjuQQ"},"cell_type":"code","source":["if False:\n","    delete_contents=False\n","    bq_client.delete_dataset(DATASET, delete_contents=delete_contents)\n","    print(f'Dataset {dataset.dataset_id} deleted from project {dataset.project}.')"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"id":"y2-E_hTejuQO","tags":[]},"cell_type":"code","source":["try:\n","    dataset = bq_client.get_dataset(dataset.dataset_id)\n","    print(f'Dataset {dataset.dataset_id} already exists '\n","          f'in location {dataset.location} in project {dataset.project}.')\n","except:\n","    dataset = bq_client.create_dataset(dataset)\n","    print(f'Dataset {dataset.dataset_id} created '\n","          f'in location {dataset.location} in project {dataset.project}.')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"Dataset riid already exists in location US in project learninggcp82.\n"}]},{"metadata":{"id":"HItb6CWGjuQS"},"cell_type":"markdown","source":["## Load Tables"]},{"metadata":{},"cell_type":"markdown","source":["# Upload Data ( Deasctivated by default)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["if False:\n","    bqh.upload_file_gcs(\"../input/riiid-test-answer-prediction/example_sample_submission.csv\")\n","    bqh.upload_file_gcs(\"../input/riiid-test-answer-prediction/example_test.csv\")\n","    bqh.upload_file_gcs(\"../input/riiid-test-answer-prediction/lectures.csv\")\n","    bqh.upload_file_gcs(\"../input/riiid-test-answer-prediction/questions.csv\")\n","    bqh.upload_file_gcs(\"../input/riiid-test-answer-prediction/train.csv\")"],"execution_count":7,"outputs":[]},{"metadata":{"id":"BjTuu8CJjuQT"},"cell_type":"markdown","source":["### Dataframe dtypes"]},{"metadata":{"trusted":true,"id":"8dxMzeCsjuQT","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","dtypes_orig = {\n","    'lectures': {\n","        'lecture_id': 'uint16',\n","        'tag': 'uint8',\n","        'part': 'uint8',\n","        'type_of': 'str',\n","    },\n","    'questions': {\n","        'question_id': 'uint16',\n","        'bundle_id': 'uint16',\n","        'correct_answer': 'uint8',\n","        'part': 'uint8',\n","        'tags': 'str',\n","        \n","    },\n","    'train': {\n","        'row_id': 'int64',\n","        'timestamp': 'int64',\n","        'user_id': 'int32',\n","        'content_id': 'int16',\n","        'content_type_id': 'int8',\n","        'task_container_id': 'int16',\n","        'user_answer': 'int8',\n","        'answered_correctly': 'int8',\n","        'prior_question_elapsed_time': 'float32', \n","        'prior_question_had_explanation': 'bool'\n","    }\n","    \n","}\n","\n","dtypes_new = {\n","    'lectures': {},\n","    'questions': {\n","        'tag__0': 'uint8',\n","        'part_correct_pct': 'uint8',\n","        'tag__0_correct_pct': 'uint8',\n","        'question_id_correct_pct': 'uint8'\n","    },\n","    'train': {\n","        'task_container_id_orig': 'int16',\n","        'answered_correctly_cumsum': 'int16',\n","        'answered_correctly_rollsum': 'int8',\n","        'answered_incorrectly': 'int8',\n","        'answered_incorrectly_cumsum': 'int16',\n","        'answered_incorrectly_rollsum': 'int8',\n","        'answered_correctly_cumsum_pct': 'int8',\n","        'answered_correctly_rollsum_pct': 'int8',\n","        'answered_correctly_content_id_cumsum': 'int16',\n","        'answered_incorrectly_content_id_cumsum': 'int16',\n","        'answered_correctly_content_id_cumsum_pct': 'int16',\n","        'answered_correctly_cumsum_upto': 'int8',\n","        'prior_question_elapsed_time_rollavg': 'float32',\n","        'lectures_cumcount': 'int16',\n","    }\n","}\n","\n","one_hot_tags = False\n","if one_hot_tags:\n","    for tag in range(189):\n","        for table_id in ['questions']:\n","            dtypes_new[table_id][f'tag_{tag:03d}'] = 'uint8'\n","\n","dtypes = {}\n","for table_id in dtypes_orig:\n","    dtypes[table_id] = {**dtypes_orig[table_id], **dtypes_new[table_id]}\n","\n","dtypes = {\n","    **dtypes['lectures'],\n","    **dtypes['questions'],\n","    **dtypes['train']\n","}"],"execution_count":8,"outputs":[]},{"metadata":{"id":"QhPzQmvUjuQV"},"cell_type":"markdown","source":["### BigQuery Table Schemas"]},{"metadata":{"trusted":true,"id":"WdW0igS4juQW","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","type_map = {\n","    'int64': 'INTEGER',\n","    'int32': 'INTEGER',\n","    'int16': 'INTEGER',\n","    'int8': 'INTEGER',\n","    'uint8': 'INTEGER',\n","    'uint16': 'INTEGER',\n","    'str': 'STRING',\n","    'bool': 'BOOL',\n","    'float32': 'FLOAT'\n","}\n","\n","schemas_orig = {table: [SchemaField(f, type_map[t]) for f, t in\n","                   fields.items()] for table, fields in dtypes_orig.items()}\n","schemas_orig['questions'][-1] = SchemaField('tags', 'INTEGER', 'REPEATED')\n","\n","schemas = {}\n","for table_id, fields in dtypes_new.items():\n","    new_fields = [SchemaField(f, type_map[t]) for\n","                  f, t in fields.items()]\n","    schemas[table_id] = schemas_orig[table_id] + new_fields"],"execution_count":9,"outputs":[]},{"metadata":{"id":"GOpOrtr9juQY"},"cell_type":"markdown","source":["### Load Tables"]},{"metadata":{"trusted":true,"id":"saboKI_rjuQc","_kg_hide-input":true,"tags":[]},"cell_type":"code","source":["# <hide-input>\n","# Load questions from local json file - can't load tags as array from csv.\n","\n","if True:\n","    bqh.del_table('questions')\n","    \n","    df_questions = pd.read_csv(f'gs://{BUCKET}/questions.csv')\n","    df_questions.tags = df_questions.tags.fillna('188')\n","    df_questions.tags = df_questions.tags.str.split()\n","    \n","    \n","    #Add new dtypes\n","    df_questions[\"part\"] = df_questions[\"part\"].astype(\"uint8\")\n","    df_questions[\"question_id\"] = df_questions[\"question_id\"].astype(\"uint16\")\n","    \n","    mlb = MultiLabelBinarizer()\n","    one_hots = (mlb.fit_transform(df_questions.tags\n","                    .apply(lambda l: [f'tag_{int(t):03d}' for t in l])))\n","    df_one_hots = pd.DataFrame(one_hots, columns = mlb.classes_)\n","    df_one_hots = df_one_hots.astype(\"uint8\")\n","    \n","    df_questions.drop(columns=[\"tags\",\"correct_answer\",\"bundle_id\"],inplace=True)\n","    df_questions = pd.concat([df_questions, df_one_hots], axis=1)\n","    \n","    schema = [SchemaField(f, type_map[t]) for f, t in list(zip(df_questions.dtypes.index,df_questions.dtypes.apply(lambda x : x.name)))]\n","    \n","    \n","    df_questions.to_json('questions.json', orient=\"records\", lines=True)\n","    del df_questions\n","    lj = bqh.load_json_file('questions', schema).result()"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"job questions_4fefdca2-816f-48ca-9ea1-03c0ddbc1eb5 started\nLoaded 13,523 rows to table questions in  5.8 sec, 2,313 per sec.\n"}]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["if True:\n","    bqh.del_table('lectures')\n","    \n","    df_lectures = pd.read_csv(f'gs://{BUCKET}/lectures.csv')\n","    mlb = MultiLabelBinarizer()\n","    df_lectures.tag = df_lectures.tag.fillna(188)\n","    one_hots = (mlb.fit_transform(df_lectures.tag.apply(lambda t: [f'tag_{int(t):03d}'])))\n","    df_one_hots = pd.DataFrame(one_hots, columns = mlb.classes_)\n","    df_one_hots = pd.DataFrame(np.append(df_one_hots.values,np.zeros((df_one_hots.shape[0],1)),axis=1),columns = list(df_one_hots.columns) + [\"tag_188\"])\n","    df_one_hots = df_one_hots.astype(\"uint8\")\n","\n","    df_dummies = pd.get_dummies(df_lectures[\"type_of\"])\n","    df_dummies.columns = [c.replace(\" \",\"_\") for c in df_dummies.columns]\n","\n","    df_lectures = pd.concat([df_lectures[[\"lecture_id\",\"part\"]],df_one_hots,df_dummies],axis=1)\n","\n","    schema = [SchemaField(f, type_map[t]) for f, t in list(zip(df_lectures.dtypes.index.str.replace(\" \",\"_\"),df_lectures.dtypes.apply(lambda x : x.name)))]\n","    \n","    \n","    df_lectures.to_json('lectures.json', orient=\"records\", lines=True)\n","    del df_lectures\n","    lj = bqh.load_json_file('lectures', schema).result()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":"job lectures_5cf6ae7f-54d6-4ce4-adf5-ba9209f7f773 started\nLoaded 418 rows to table lectures in  1.8 sec, 234 per sec.\n"}]},{"metadata":{"trusted":true,"id":"XXwb4YkWjuQe","_kg_hide-input":true,"tags":[]},"cell_type":"code","source":["# <hide-input>\n","if True:\n","    #Load train from cloud storage\n","    bqh.del_table(\"train\")\n","    lj = bqh.load_csv_uri(\"train\").result()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"job train_d249a3f9-4dbb-4937-959d-6f2c6cd61178 started\nLoaded 101,230,332 rows to table train in 96.2 sec, 1,052,498 per sec.\n"}]},{"metadata":{"trusted":true,"id":"CHkK8JhQjuQg","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","df_jobs = bqh.get_df_jobs()\n","df_jobs"],"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["bqh.update_table_schema(\"train\",[SchemaField(\"test\",\"INTEGER\",\"NULLABLE\")])"]},{"metadata":{"trusted":true,"id":"5cGVHAtMjuQi","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","df_table_list = bqh.get_df_table_list()\n","df_table_list"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":"           table_id  cols       rows        kb\n0      example_test    11        104         7\n1          feat_tab    24   98602109  16824833\n2  feat_tab_profile    25   98602109  17613650\n3          lectures   158        418       528\n4      q_difficulty     3      13523       324\n5         questions   191      13523     20663\n6             train    10  101230332   7370609","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>table_id</th>\n      <th>cols</th>\n      <th>rows</th>\n      <th>kb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>example_test</td>\n      <td>11</td>\n      <td>104</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>feat_tab</td>\n      <td>24</td>\n      <td>98602109</td>\n      <td>16824833</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>feat_tab_profile</td>\n      <td>25</td>\n      <td>98602109</td>\n      <td>17613650</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>lectures</td>\n      <td>158</td>\n      <td>418</td>\n      <td>528</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q_difficulty</td>\n      <td>3</td>\n      <td>13523</td>\n      <td>324</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>questions</td>\n      <td>191</td>\n      <td>13523</td>\n      <td>20663</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>train</td>\n      <td>10</td>\n      <td>101230332</td>\n      <td>7370609</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":14}]},{"metadata":{"id":"sgTZkanIjuQk"},"cell_type":"markdown","source":["### Update Table Schemas"]},{"metadata":{"trusted":true,"id":"_t7T4NftjuQk","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","if False:\n","    for table_id, schema in schemas.items():\n","        table = bqh.get_table(table_id)\n","        table.schema = schema\n","        table = bq_client.update_table(table, ['schema'])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Ouoo1oQCjuQm"},"cell_type":"markdown","source":["## Engineer Features"]},{"metadata":{"id":"CU8m9kx9juQn"},"cell_type":"markdown","source":["A good workflow here is:\n","* Create a sample of the train table.\n","* Use the BigQuery query editor user interface to get the SQL for a new feature worked out as a selection from the `train_sample` table. The user interface there has tab completion, syntax checking and displays results, which makes creating and debugging queries a snap.\n","    * [BigQuery Console](https://console.cloud.google.com/bigquery?project=riiid-caleb) (Update project query string for your project.)\n","    * [BigQuery Query syntax in Standard SQL](https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax) is your friend.\n","* Optional: create a local dataframe, using the export functions below, to confirm that it is working the right way.\n","* Add a column to the appropriate table by adding a value to `dtypes_new`\n","* Update the schema for the table in BigQuery by running the Update Table Schemas cell above\n","* Recreate the `train_sample` table by running the cell below.\n","* Use the BigQuery query editor user interface add the logic to update the new column.\n","* Optional: create a local dataframe, using the export functions below, to confirm that the update is working the right way.\n","* Copy the SQL to a new method in the `Queries` class above\n","* Add the query to the appropriate `run_transformations` function above\n","* Run transformations on `train_sample` table\n","* Inspect `train_sample` table in BigQuery to confirm everything is working correctly\n","* Optional: load load local dataframe using `get_df_query` function for further inspection\n","* Run transformations on `train` table\n","* Inspect `train` table in BigQuery to confirm everything is working correctly\n","* Optional: load local dataframe using `get_df_query` function for further inspection"]},{"metadata":{"id":"CfKY_WsvjuQp"},"cell_type":"markdown","source":["### Perform Transformations"]},{"metadata":{"id":"ZOnBP6sfjuQp"},"cell_type":"markdown","source":["#### Train Table\n","* Add question columns\n","    * Adding question part and the first associated tag. (There wasn't any official information regarding the order of the tags as recorded for each question, but they did not appear to be sorted so it seems possible the order in which they are recorded is significant.)\n","* Update task_container_id to increase monotonically with timestamp\n","    * There were some `task_conatiner_id`s that were out of order with respect to timestamp. They needed to be be ordered correctly so that cumulative and rolling sums partitioned by `task_container_id` would be include only interactions with earlier `timestamps`. Even though all interactions with the same `task_container_id` have the same `timestamp`, partioning by `timestamp` is much slower (because the range of values is so much wider?).\n","* Calc answered_incorrectly\n","    * `answered_correctly` for lectures was recorded as -1 and needed to be set to 0 to calculate cumulative and rolling sums correctly including lectures. As a consequence, `answered_incorrectly` could be calculated as the inverse of `answered_correctly`.\n","* Calc cumsum for `answered_correctly` and `answered_incorrectly` by `user_id` and by `user_id` and `content_id` and rolling avg for `prior_question_elapsed_time` by user \n","    * This is done so that the totals are as of the preceding `task_container_id`\n","* Calculate rolling sum for `answered_correctly` and `answered_incorrectly` by `user_id`\n","    * Includes the 10 rows preceding the current `task_container_id`\n","    * I couldn't figure out how to get this done with the standard window functionality since I wanted a set number of rows preceding the current task container (as opposed to just the current row), so it joins on `user_id` with a `task_container_id` less than the current one, which takes a while to complete.\n","* Calculate answered correctly percentages for `answered_correctly_cumsum`, `answered_correctly_rollsum` and `answered_correctly_content_id_cumsum_pct`\n","\n","#### Questions Table\n","* Calculate percent answered correctly for `question_id`, `part` and `tag__0` "]},{"metadata":{"id":"ndY56UJdhrIU","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":["# <hide-input>\n","cumsum_pct_specs = [\n","    dict(column_id_correct='answered_correctly_cumsum',\n","         column_id_incorrect='answered_incorrectly_cumsum',\n","         update_column_id='answered_correctly_cumsum_pct'),\n","    \n","    dict(column_id_correct='answered_correctly_rollsum',\n","         column_id_incorrect='answered_incorrectly_rollsum',\n","         update_column_id='answered_correctly_rollsum_pct'),\n","    \n","    dict(column_id_correct='answered_correctly_content_id_cumsum',\n","         column_id_incorrect='answered_incorrectly_content_id_cumsum',\n","         update_column_id='answered_correctly_content_id_cumsum_pct'),                   \n","]\n","\n","def run_update_correct_cumsum_pct(spec):\n","    query, job_id_prefix = Q.update_correct_cumsum_pct(**spec)\n","    job_id_prefix = f'{job_id_prefix}{spec[\"update_column_id\"]}_'\n","    bqh.run_query(query=query, job_id_prefix=job_id_prefix, wait=True)"],"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"('\\n            UPDATE riid.train t\\n            SET task_container_id = target.calc\\n            FROM (\\n              SELECT row_id, DENSE_RANK()\\n                OVER (\\n                  PARTITION BY user_id\\n                  ORDER BY timestamp\\n                ) - 1 calc\\n              FROM riid.train\\n            ) target\\n            WHERE target.row_id = t.row_id\\n        ',\n 'update_task_container_id_')"},"metadata":{},"execution_count":59}],"source":["Q.update_task_container_id(table_id=table_id)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def run_train_cleanup():\n","\n","    #Run serially\n","    train_queries = [\n","        Q.update_task_container_id(table_id=table_id),\n","        Q.update_answered_incorrectly(table_id=table_id),\n","        Q.update_missing_values(table_id=table_id,\n","                                column_id='prior_question_had_explanation',\n","                                value='false'),\n","        Q.update_missing_values(table_id=table_id,\n","                                column_id='prior_question_elapsed_time',\n","                                value='0')\n","    ]\n","    \n","    _ = [bqh.run_query(*q, wait=True) for q in train_queries]"]},{"metadata":{"trusted":true,"id":"r2LZdBETjuQs","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","def run_train_transforms(table_id=None):\n","    # Run serially to avoid update conflicts\n","    \n","    train_queries = [\n","        Q.update_task_container_id(table_id=table_id),\n","        Q.update_answered_incorrectly(table_id=table_id),\n","        Q.update_missing_values(table_id=table_id,\n","                                column_id='prior_question_had_explanation',\n","                                value='false'),\n","        Q.update_missing_values(table_id=table_id,\n","                                column_id='prior_question_elapsed_time',\n","                                value='0'),\n","        Q.update_train_window_containers(table_id=table_id),\n","        Q.update_train_window_rows(table_id=table_id, window=10),\n","        Q.update_answered_correctly_cumsum_upto(table_id=table_id)\n","    ]\n","    \n","    _ = [bqh.run_query(*q, wait=True) for q in train_queries]\n","\n","    _ = [spec.update(table_id=table_id) for spec in cumsum_pct_specs]\n","    _ = list(map(run_update_correct_cumsum_pct, cumsum_pct_specs))"],"execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"id":"8Ay24JcgjuQv","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","def run_questions_transforms():\n","    \"\"\"These have to be run after the transforms are run on the full\n","    train table.\n","    \"\"\"\n","    \n","    questions_queries = [Q.update_questions_tag__0()]\n","    for column_id in ['question_id', 'part', 'tag__0']:\n","        questions_queries.append(Q.update_question_correct_pct(column_id))\n","    \n","    _ = [bqh.run_query(*q, wait=True).result() for q in questions_queries]"],"execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"id":"tuh_k3VfjuQx","_kg_hide-input":true,"tags":[]},"cell_type":"code","source":["# <hide-input>\n","if True:\n","    run_train_transforms('train')\n","    run_questions_transforms()"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":"Job update_task_container_id_1bdb0b47-4384-4d6f-87f4-845a28e5f27a started.\nJob update_task_container_id_1bdb0b47-4384-4d6f-87f4-845a28e5f27a finished in 94.838 seconds.\nJob update_answered_incorrectly_8b059f36-947f-4756-b7a1-d26243a9f828 started.\nJob update_answered_incorrectly_8b059f36-947f-4756-b7a1-d26243a9f828 finished in 0.504 seconds.\n"},{"output_type":"error","ename":"BadRequest","evalue":"400 Query error: Unrecognized name: answered_incorrectly; Did you mean answered_correctly? at [3:17]\n\n(job ID: update_answered_incorrectly_8b059f36-947f-4756-b7a1-d26243a9f828)\n\n                 -----Query Job SQL Follows-----                  \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:            UPDATE riid.train\n   3:            SET answered_incorrectly = 0\n   4:            WHERE true;\n   5:\n   6:            UPDATE riid.train\n   7:            SET answered_incorrectly = 1 - answered_correctly\n   8:            WHERE content_type_id = 0;\n   9:\n  10:            UPDATE riid.train\n  11:            SET answered_correctly = 0\n  12:            WHERE content_type_id = 1;\n  13:        \n    |    .    |    .    |    .    |    .    |    .    |    .    |","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-518eca564faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# <hide-input>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_train_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrun_questions_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-45a7b6bbeed1>\u001b[0m in \u001b[0;36mrun_train_transforms\u001b[0;34m(table_id)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbqh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_queries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcumsum_pct_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-45a7b6bbeed1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbqh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_queries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcumsum_pct_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-a2a683f28629>\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, job_id_prefix, wait)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index)\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[1;32m   3229\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3230\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3232\u001b[0m             \u001b[0;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequest\u001b[0m: 400 Query error: Unrecognized name: answered_incorrectly; Did you mean answered_correctly? at [3:17]\n\n(job ID: update_answered_incorrectly_8b059f36-947f-4756-b7a1-d26243a9f828)\n\n                 -----Query Job SQL Follows-----                  \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:            UPDATE riid.train\n   3:            SET answered_incorrectly = 0\n   4:            WHERE true;\n   5:\n   6:            UPDATE riid.train\n   7:            SET answered_incorrectly = 1 - answered_correctly\n   8:            WHERE content_type_id = 0;\n   9:\n  10:            UPDATE riid.train\n  11:            SET answered_correctly = 0\n  12:            WHERE content_type_id = 1;\n  13:        \n    |    .    |    .    |    .    |    .    |    .    |    .    |"]}]},{"cell_type":"code","execution_count":38,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Job update_task_container_id_e57e6a0e-5e7c-4dce-9ea5-5f86b0b036ea started.\nJob update_task_container_id_e57e6a0e-5e7c-4dce-9ea5-5f86b0b036ea finished in 95.533 seconds.\nJob update_answered_incorrectly_a4fa2ec2-5041-4fb2-8ef7-b55fba45aa16 started.\nJob update_answered_incorrectly_a4fa2ec2-5041-4fb2-8ef7-b55fba45aa16 finished in 0.298 seconds.\n"},{"output_type":"error","ename":"BadRequest","evalue":"400 Query error: Unrecognized name: answered_incorrectly; Did you mean answered_correctly? at [3:17]\n\n(job ID: update_answered_incorrectly_a4fa2ec2-5041-4fb2-8ef7-b55fba45aa16)\n\n                 -----Query Job SQL Follows-----                  \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:            UPDATE riid.train\n   3:            SET answered_incorrectly = 0\n   4:            WHERE true;\n   5:\n   6:            UPDATE riid.train\n   7:            SET answered_incorrectly = 1 - answered_correctly\n   8:            WHERE content_type_id = 0;\n   9:\n  10:            UPDATE riid.train\n  11:            SET answered_correctly = 0\n  12:            WHERE content_type_id = 1;\n  13:        \n    |    .    |    .    |    .    |    .    |    .    |    .    |","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-b6a7ad996696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run train cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_train_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-3d68d248a422>\u001b[0m in \u001b[0;36mrun_train_cleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbqh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_queries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-3d68d248a422>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbqh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_queries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-a2a683f28629>\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, job_id_prefix, wait)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index)\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[1;32m   3229\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3230\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3232\u001b[0m             \u001b[0;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/cloud/bigquery/job.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequest\u001b[0m: 400 Query error: Unrecognized name: answered_incorrectly; Did you mean answered_correctly? at [3:17]\n\n(job ID: update_answered_incorrectly_a4fa2ec2-5041-4fb2-8ef7-b55fba45aa16)\n\n                 -----Query Job SQL Follows-----                  \n\n    |    .    |    .    |    .    |    .    |    .    |    .    |\n   1:\n   2:            UPDATE riid.train\n   3:            SET answered_incorrectly = 0\n   4:            WHERE true;\n   5:\n   6:            UPDATE riid.train\n   7:            SET answered_incorrectly = 1 - answered_correctly\n   8:            WHERE content_type_id = 0;\n   9:\n  10:            UPDATE riid.train\n  11:            SET answered_correctly = 0\n  12:            WHERE content_type_id = 1;\n  13:        \n    |    .    |    .    |    .    |    .    |    .    |    .    |"]}],"source":["# Run train cleanup\n","run_train_cleanup()"]},{"metadata":{"id":"YTG_h2a2juQy"},"cell_type":"markdown","source":["### Check Output"]},{"metadata":{"trusted":true,"id":"12-1u5TdjuQ1","tags":[]},"cell_type":"code","source":["query = Q.select_train(table_id='train', excl_lectures=True)\n","df_query = bqh.get_df_query(query, dtypes=dtypes)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"Job select_train_578a6d1d-38f3-4722-83e1-2e4c3317a998 started.\nJob select_train_578a6d1d-38f3-4722-83e1-2e4c3317a998 finished in 11.202 seconds.\n"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=8674.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76837d545b34e6d8c58011cb383210f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_query"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":"      row_id    timestamp  user_id  content_id  content_type_id  \\\n0          2       118363      115         128                0   \n1          0            0      115        5692                0   \n2          1        56943      115        5716                0   \n3          3       131167      115        7860                0   \n4          4       137965      115        7922                0   \n...      ...          ...      ...         ...              ...   \n8669    8875  22050548541    46886        8180                0   \n8670    8876  22050565680    46886        3693                0   \n8671    8877  22050584842    46886        5397                0   \n8672    8878  22050610311    46886        5326                0   \n8673    8879  22050635752    46886        9517                0   \n\n      task_container_id  user_answer  answered_correctly  \\\n0                     0            0                   1   \n1                     1            3                   1   \n2                     2            2                   1   \n3                     3            0                   1   \n4                     4            1                   1   \n...                 ...          ...                 ...   \n8669                 40            1                   1   \n8670                 41            0                   1   \n8671                 42            1                   1   \n8672                 43            1                   1   \n8673                 44            3                   0   \n\n      prior_question_elapsed_time  prior_question_had_explanation  ...  \\\n0                         55000.0                           False  ...   \n1                             NaN                           False  ...   \n2                         37000.0                           False  ...   \n3                         19000.0                           False  ...   \n4                         11000.0                           False  ...   \n...                           ...                             ...  ...   \n8669                       7000.0                            True  ...   \n8670                      15000.0                            True  ...   \n8671                       9000.0                            True  ...   \n8672                      12000.0                            True  ...   \n8673                      17000.0                            True  ...   \n\n      tag_179  tag_180  tag_181  tag_182  tag_183  tag_184  tag_185  tag_186  \\\n0           0        0        0        0        0        0        0        0   \n1           0        0        0        0        0        0        0        0   \n2           0        0        0        0        0        0        0        0   \n3           0        0        0        0        0        0        0        0   \n4           0        0        0        0        0        0        0        0   \n...       ...      ...      ...      ...      ...      ...      ...      ...   \n8669        0        0        0        0        0        0        0        0   \n8670        0        0        0        0        0        0        0        0   \n8671        0        0        0        0        0        0        0        0   \n8672        0        0        0        0        0        0        0        0   \n8673        0        0        0        0        0        0        0        0   \n\n      tag_187  tag_188  \n0           0        0  \n1           0        0  \n2           0        0  \n3           0        0  \n4           0        0  \n...       ...      ...  \n8669        0        0  \n8670        0        0  \n8671        0        0  \n8672        0        0  \n8673        0        0  \n\n[8674 rows x 201 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>timestamp</th>\n      <th>user_id</th>\n      <th>content_id</th>\n      <th>content_type_id</th>\n      <th>task_container_id</th>\n      <th>user_answer</th>\n      <th>answered_correctly</th>\n      <th>prior_question_elapsed_time</th>\n      <th>prior_question_had_explanation</th>\n      <th>...</th>\n      <th>tag_179</th>\n      <th>tag_180</th>\n      <th>tag_181</th>\n      <th>tag_182</th>\n      <th>tag_183</th>\n      <th>tag_184</th>\n      <th>tag_185</th>\n      <th>tag_186</th>\n      <th>tag_187</th>\n      <th>tag_188</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>118363</td>\n      <td>115</td>\n      <td>128</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>55000.0</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>115</td>\n      <td>5692</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>56943</td>\n      <td>115</td>\n      <td>5716</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>37000.0</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>131167</td>\n      <td>115</td>\n      <td>7860</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>19000.0</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>137965</td>\n      <td>115</td>\n      <td>7922</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11000.0</td>\n      <td>False</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8669</th>\n      <td>8875</td>\n      <td>22050548541</td>\n      <td>46886</td>\n      <td>8180</td>\n      <td>0</td>\n      <td>40</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7000.0</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8670</th>\n      <td>8876</td>\n      <td>22050565680</td>\n      <td>46886</td>\n      <td>3693</td>\n      <td>0</td>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>15000.0</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8671</th>\n      <td>8877</td>\n      <td>22050584842</td>\n      <td>46886</td>\n      <td>5397</td>\n      <td>0</td>\n      <td>42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9000.0</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8672</th>\n      <td>8878</td>\n      <td>22050610311</td>\n      <td>46886</td>\n      <td>5326</td>\n      <td>0</td>\n      <td>43</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12000.0</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8673</th>\n      <td>8879</td>\n      <td>22050635752</td>\n      <td>46886</td>\n      <td>9517</td>\n      <td>0</td>\n      <td>44</td>\n      <td>3</td>\n      <td>0</td>\n      <td>17000.0</td>\n      <td>True</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8674 rows  201 columns</p>\n</div>"},"metadata":{},"execution_count":23}]},{"metadata":{"trusted":true,"id":"ub-U0nPgjuQ3","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","cols = [\n","        'row_id',\n","        'task_container_id_orig',\n","        'timestamp',\n","        'content_type_id',\n","        'user_id',\n","        'task_container_id',\n","        'part',\n","        'tag__0',\n","        'answered_correctly',\n","        'answered_incorrectly',\n","        'answered_correctly_cumsum',\n","        'answered_incorrectly_cumsum',\n","        'answered_correctly_content_id_cumsum',\n","        'answered_correctly_rollsum',\n","        'answered_incorrectly_rollsum',\n","        'answered_incorrectly_content_id_cumsum',\n","        'part_correct_pct',\n","        'tag__0_correct_pct',\n","        'question_id_correct_pct',\n","        'prior_question_elapsed_time',\n","        'prior_question_elapsed_time_rollavg',\n","        'prior_question_had_explanation',\n","        'lectures_cumcount',\n","        'answered_correctly_cumsum_upto'\n","]\n","\n","df_user = df_query[cols].copy()\n","df_user.timestamp = df_user.timestamp / (1000*60*60)\n","\n","df_user.loc[df_user.user_id == 44331].head(20)"],"execution_count":24,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"\"['part_correct_pct', 'tag__0_correct_pct', 'answered_incorrectly_rollsum', 'answered_correctly_rollsum', 'answered_correctly_content_id_cumsum', 'answered_correctly_cumsum', 'task_container_id_orig', 'answered_incorrectly', 'lectures_cumcount', 'answered_incorrectly_cumsum', 'prior_question_elapsed_time_rollavg', 'question_id_correct_pct', 'answered_incorrectly_content_id_cumsum', 'answered_correctly_cumsum_upto', 'tag__0'] not in index\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-3ae85be64ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdf_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_user\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/riid_kaggle/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['part_correct_pct', 'tag__0_correct_pct', 'answered_incorrectly_rollsum', 'answered_correctly_rollsum', 'answered_correctly_content_id_cumsum', 'answered_correctly_cumsum', 'task_container_id_orig', 'answered_incorrectly', 'lectures_cumcount', 'answered_incorrectly_cumsum', 'prior_question_elapsed_time_rollavg', 'question_id_correct_pct', 'answered_incorrectly_content_id_cumsum', 'answered_correctly_cumsum_upto', 'tag__0'] not in index\""]}]},{"metadata":{"id":"zO6GM_CLjuQ5"},"cell_type":"markdown","source":["### Visually Inspect Features"]},{"metadata":{"id":"jpokxssbjuQ6"},"cell_type":"markdown","source":["The charts below can also be used to visually inspect whether the transformations have been performed correctly."]},{"metadata":{"trusted":true,"id":"0c-xlk2TjuQ6","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","groups = {\n","    'cum': {\n","        'columns': {\n","            'task_container_id': 0,\n","            'answered_correctly_cumsum': 2,\n","            'answered_incorrectly_cumsum': 1\n","        },\n","        'xaxis': 'elapsed_hours'\n","    },\n","    'roll': {\n","        'columns': {\n","            'answered_correctly_rollsum': 2,\n","            'answered_correctly': 7,\n","            'answered_incorrectly_rollsum': 1,\n","            'answered_incorrectly': 8,\n","            'part': 9\n","        },\n","        'xaxis': 'row_id'\n","    },  \n","    'correct_pct': {\n","        'columns': {\n","            'question_id_correct_pct': 0,\n","            'part_correct_pct': 5,\n","            'tag__0_correct_pct': 6\n","        },\n","        'xaxis': 'row_id'\n","    },  \n","    'prior_question_elapsed_time': {\n","        'columns': {\n","            'prior_question_elapsed_time': 0,\n","        },\n","        'xaxis': 'row_id'\n","    },  \n","    'prior_question_had_explanation': {\n","        'columns': {\n","            'prior_question_had_explanation': 0,\n","        },\n","        'xaxis': 'row_id'\n","    }\n","}\n","\n","def plot_user_learning(user_id=None, group=None, suffix=None):\n","    theme = px.colors.qualitative.Plotly\n","    columns = list(group['columns'].keys())\n","    colors = [theme[c] for c in group['columns'].values()]\n","\n","    df_query['elapsed_hours'] = df_query.timestamp / (1000*60*60)\n","\n","    df = (df_query.loc[(df_user.user_id == user_id) &\n","                       (df_user.content_type_id == 0)])\n","\n","    # labels = {'value': 'answer count'}\n","\n","    fig = df.plot(x=group['xaxis'], y=columns, color_discrete_sequence=colors,\n","                  title=f'Learning Progress - user_id = {user_id} - {suffix}')\n","    fig.data\n","\n","    return fig\n","\n","user_id_random = np.random.choice(df_query.user_id.unique(), (1,))[0]\n","use_random = False\n","user_id =  user_id_random if use_random else 5382\n","\n","for k, v in groups.items():\n","    fig = plot_user_learning(user_id, v, k)\n","    fig.show()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"F_B3C-9QjuQ9"},"cell_type":"markdown","source":["### Create Sample of Train Table for R&D"]},{"metadata":{"trusted":true,"id":"tuNFCT1OjuQ9","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","if False:\n","    bqh.run_query(*Q.create_train_sample(), wait=True)\n","    q = Q.select_train(excl_lectures=True, table_id='train_sample')\n","    df_sample = bqh.get_df_query(q)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"wbaxOZFVjuRA"},"cell_type":"markdown","source":["## Create Local Training Dataframe"]},{"metadata":{"id":"7GBDKOZ2juRA"},"cell_type":"markdown","source":["With feature engineering being performed in BigQuery, data has to be exported to train models locally. The [Python Client for Google BigQuery](https://googleapis.dev/python/bigquery/latest/index.html) [to_dataframe()](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJob.html?highlight=to_dataframe#google.cloud.bigquery.job.QueryJob.to_dataframe) makes it possible to create dataframes directly, but is prohibitively slow for large datasets. While it is not possible to export table directly to the local file system, it is possible to export to cloud storage and then download locally from there. This is reasonably efficient, taking a couple of minutes to run a query, export to cloud storage, download to the local file system and then read the files into a dataframe. The is another api, the [BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage), that a client can be created with that is really fast and works with the `to_dataframe` method, but unforunatley it isn't working with the current Kaggle kernel environment.\n","\n","The functions below take advantage of the fact BigQuery stores queries in temporary tables so that preveiously requested queries can be retrieved without having to run them again. Similarly, the functions below name the exported files with the reference to the BigQuery temporary table, so that if a function is run to create a dataframe from a query for which the files already exist in cloud storage or locally, they won't be exported or downloaded again. "]},{"metadata":{"id":"273_RZ1StVS0"},"cell_type":"markdown","source":["### Create DataFrame"]},{"metadata":{"trusted":true,"id":"QUkacRjajuRG","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","features = {\n","    'answered_correctly':                       True,\n","    'answered_correctly_content_id_cumsum':     True,\n","    'answered_correctly_content_id_cumsum_pct': True,\n","    'answered_correctly_cumsum':                True,\n","    'answered_correctly_cumsum_upto':           True,\n","    'answered_correctly_cumsum_pct':            True,\n","    'answered_correctly_rollsum':               True,\n","    'answered_correctly_rollsum_pct':           True,\n","    'answered_incorrectly':                     True,\n","    'answered_incorrectly_content_id_cumsum':   True,\n","    'answered_incorrectly_cumsum':              True,\n","    'answered_incorrectly_rollsum':             True,\n","    'bundle_id':                                False,\n","    'content_id':                               True,\n","    'content_type_id':                          True,\n","    'correct_answer':                           False,\n","    'lecture_id':                               False,\n","    'lectures_cumcount':                        True,\n","    'part':                                     True,\n","    'part_correct_pct':                         True,\n","    'prior_question_elapsed_time':              True,\n","    'prior_question_elapsed_time_rollavg':      True,\n","    'prior_question_had_explanation':           True,\n","    'question_id':                              False,\n","    'question_id_correct_pct':                  True,\n","    'row_id':                                   True,\n","    'tag':                                      False,\n","    'tag__0':                                   True,\n","    'tag__0_correct_pct':                       True,\n","    'tags':                                     False,\n","    'task_container_id':                        True,\n","    'task_container_id_orig':                   False,\n","    'timestamp':                                True,\n","    'type_of':                                  False,\n","    'user_answer':                              False,\n","    'user_id':                                  True\n","}\n","\n","tag_cols = [f'tag_{tag:03d}' for tag in range(189)] if one_hot_tags else []\n","\n","columns_export = [f for f, v in features.items() if v]\n","if one_hot_tags:\n","    columns_export = columns_export +  tag_cols"],"execution_count":null,"outputs":[]},{"metadata":{"id":"32Wt1DvjpnJZ","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","def get_features_widget(features_dict, columns_list):\n","\n","    names = []\n","    widget_list = []\n","    for key, v in features_dict.items():\n","        widget_list.append(widgets.ToggleButton(value=v,\n","                                                description=key,\n","                                                layout={'width': '290px'},\n","                                                button_style='primary'))\n","        names.append(key)\n","\n","    arg_dict = {names[i]: widget for i, widget in enumerate(widget_list)}\n","\n","    layout = widgets.Layout(grid_template_columns=\"repeat(3, 300px)\")\n","    ui = widgets.GridBox(widget_list, layout=layout)\n","\n","    def select_data(**kwargs):\n","        columns_list.clear()\n","\n","        for key in kwargs:\n","            features_dict[key] = False\n","            if kwargs[key]:\n","                columns_list.append(key)\n","                features_dict[key] = True\n","\n","        print(f'{len(columns_list)} columns selected')\n","\n","    output = widgets.interactive_output(select_data, arg_dict)\n","    return ui, output"],"execution_count":null,"outputs":[]},{"metadata":{"id":"B6ddqQ4HBT3R","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","columns_export = []\n","display(*get_features_widget(features, columns_export))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"7eS8EvZGjuRI","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","# get user_ids for rows in thousands. will be approximate, excludes lectures\n","# and selects all records for user_ids less than specified.\n","\n","if False:    \n","    r = Q.run_query(*Q.select_user_id_rows(rows=int(2e6))).result()\n","    user_id_max = list(r)[0].user_id\n","    print(user_id_max)\n","    \n","user_ids = {\n","    10: 91216,\n","    100: 2078569,\n","    1000: 20949024,\n","    2000: 42207371,\n","    10000: 216747867,\n","    30000: 643006676\n","}"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EXRXQ86UjuRK","_kg_hide-output":true},"cell_type":"code","source":["# <hide-output>\n","if True:\n","    query = Q.select_train(columns=columns_export, user_id_max=user_ids[10000],\n","                           excl_lectures=True)\n","    df_train = bqh.get_df_query_gcs(query, dtypes=dtypes, file_format='json')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"oMqolxVmjuRL"},"cell_type":"markdown","source":["## Train Model"]},{"metadata":{"id":"Axb2oD77juRM"},"cell_type":"markdown","source":["### Create Train and Validation Splits"]},{"metadata":{"id":"jNsk1_JsjuRN"},"cell_type":"markdown","source":["This is a first pass at a validation split to be able to have something to get the mechanics of evaluating the model up and running. It simply takes the last 20 `task_container_id`s for each user. The result is that all of the records in the validation set have `task_container_ids` greater than those in the training set for each user. There are also users in the validation set that are not present in the training set. However, a significant problem with this methodology is that the number of records per user in the validation set is much lower than it is in the training set."]},{"metadata":{"trusted":true,"id":"E791NgaXjuRN","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","# get unique user_id-task_container_id combinations\n","df_user_task = df_train.groupby(['user_id',\n","                                 'task_container_id'])[['user_id',\n","                                                        'task_container_id',\n","                                                        'row_id']].head(1)\n","\n","# get index of trailing number of unique user_id-task_container_id combinations\n","index_valid = (df_user_task.groupby('user_id').tail(20)\n","               .set_index(['user_id', 'task_container_id']).index)\n","\n","# use index to get ids of all rows in the chosen set of user_id-task_container\n","# combinations\n","row_valid = (df_train.set_index(['user_id', 'task_container_id'])['row_id']\n","             .loc[index_valid].values)\n","\n","df_train['valid'] = df_train.row_id.isin(row_valid)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dUhlNdidjuRP","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","title = 'Train and Validation Splits - Record Counts'\n","df_train.valid.value_counts().plot(kind='bar', title=title)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"FNj-Z1RajuRQ","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","(df_train.groupby(['valid','user_id'])[['valid','user_id']].head(1)\n"," .reset_index().groupby('valid').count().user_id\n"," .plot(kind='bar', title='Count of Users by Split'))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"h98GAgwRjuRU","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","g_user_ct = (df_train[['valid', 'row_id', 'user_id']]\n","             .groupby(['valid', 'user_id']).count())\n","\n","bins = [0,10,20,50,100,250,500,1000,2500,5000,20000]\n","g_user_ct['bin'] = pd.cut(g_user_ct.row_id, bins=bins, duplicates='drop')\n","g_counts = (g_user_ct.reset_index()\n","            .groupby(['valid', 'bin'])['row_id'].count().reset_index())\n","\n","px.bar(x=g_counts.bin.apply(str), y=g_counts.row_id,\n","       facet_col=g_counts.valid.map({True: 'Validation', False: 'Train'}),\n","       title='Count of Users by Count of Interactions by Split',\n","       labels={'x': 'Count of Interactions',\n","               'y': 'Count of Users',\n","               'facet_col': 'Validation Split'})"],"execution_count":null,"outputs":[]},{"metadata":{"id":"8VM6n9FKyczN"},"cell_type":"markdown","source":["### Select Columns for Training"]},{"metadata":{"id":"Kr1aADasnT-M","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","features_train = {\n","    'answered_correctly':                       False,\n","    'answered_correctly_content_id_cumsum':     True,\n","    'answered_correctly_content_id_cumsum_pct': False,\n","    'answered_correctly_cumsum':                True,\n","    'answered_correctly_cumsum_upto':           False,\n","    'answered_correctly_cumsum_pct':            True,\n","    'answered_correctly_rollsum':               False,\n","    'answered_correctly_rollsum_pct':           False,\n","    'answered_incorrectly':                     False,\n","    'answered_incorrectly_content_id_cumsum':   True,\n","    'answered_incorrectly_cumsum':              True,\n","    'answered_incorrectly_rollsum':             False,\n","    'bundle_id':                                False,\n","    'content_id':                               False,\n","    'content_type_id':                          False,\n","    'correct_answer':                           False,\n","    'lecture_id':                               False,\n","    'lectures_cumcount':                        False,\n","    'part':                                     True,\n","    'part_correct_pct':                         True,\n","    'prior_question_elapsed_time':              False,\n","    'prior_question_elapsed_time_rollavg':      False,\n","    'prior_question_had_explanation':           False,\n","    'question_id':                              False,\n","    'question_id_correct_pct':                  True,\n","    'row_id':                                   False,\n","    'tag':                                      False,\n","    'tag__0':                                   True,\n","    'tag__0_correct_pct':                       True,\n","    'tags':                                     False,\n","    'task_container_id':                        True,\n","    'task_container_id_orig':                   False,\n","    'timestamp':                                True,\n","    'type_of':                                  False,\n","    'user_answer':                              False,\n","    'user_id':                                  False\n","    }\n","\n","columns_train = [f for f, v in features_train.items() if v] + tag_cols"],"execution_count":null,"outputs":[]},{"metadata":{"id":"MUSDoZoLEKP_","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","columns_train = []\n","display(*get_features_widget(features_train, columns_train))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"q-zrGOfDwjw9","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","def show_features():\n","    df_features = pd.DataFrame([features, features_train]).T.reset_index()\n","    df_features.columns = ['feature', 'export', 'train']\n","    df_features\n","\n","    def highlight_true(s):\n","        return ['background-color: lightskyblue' if v else '' for v in s]\n","    return df_features.style.apply(highlight_true, subset=['export', 'train'])\n","show_features()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"reut9PmcjuRV","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","y_train_col = ['answered_correctly']\n","\n","x_train_cols = columns_train\n","\n","train_matrix = xgb.DMatrix(data=df_train.loc[~df_train.valid][x_train_cols],\n","                           label=df_train.loc[~df_train.valid][y_train_col])\n","\n","valid_matrix = xgb.DMatrix(data=df_train.loc[df_train.valid][x_train_cols],\n","                           label=df_train.loc[df_train.valid][y_train_col])"],"execution_count":null,"outputs":[]},{"metadata":{"id":"E5gUTzNtjuRX"},"cell_type":"markdown","source":["### Train Model"]},{"metadata":{"trusted":true,"id":"t6AeR8lBjuRX","_kg_hide-output":true},"cell_type":"code","source":["# <hide-output>\n","params = {\n","    'eta': 0.2,\n","    'max_depth': 6,\n","    'max_bin': 256,\n","    'tree_method': 'gpu_hist',\n","    'grow_policy': 'lossguide',\n","    'sampling_method': 'gradient_based',\n","    'objective': 'binary:logistic',\n","    'eval_metric': ['error', 'logloss', 'auc']\n","}\n","\n","if NOT_KAGGLE:\n","    experiment = Experiment()\n","\n","evals_result = {}\n","model = xgb.train(params=params, dtrain=train_matrix, num_boost_round=300,\n","                  evals=[(train_matrix, 'train'), (valid_matrix, 'valid')],\n","                  evals_result=evals_result, early_stopping_rounds=10)\n","\n","if NOT_KAGGLE:\n","    experiment.end()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"fe7JKQf0juRZ"},"cell_type":"markdown","source":["## Evaluate Model"]},{"metadata":{"trusted":true,"id":"qPTKUFJMjuRZ","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","def get_evals_df(evals_result):\n","    evals_list = []\n","    for k,v in evals_result.items():\n","        for j,u in v.items():\n","            evals_list.extend([{'epoch': i,\n","                                'split': k,\n","                                'metric': j,\n","                                'result': r} for i,r in enumerate(u)])\n","    \n","    df_evals = (pd.DataFrame(evals_list).set_index(['split', 'metric', 'epoch'])\n","                .unstack('metric'))\n","    df_evals.columns = df_evals.columns.get_level_values(1)\n","    df_evals.columns.name = None\n","    \n","    return df_evals.reset_index()\n","\n","df_evals = get_evals_df(evals_result)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"oa7h74fSjuRb","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","df_evals.plot(x='epoch', y=['auc', 'logloss'],\n","              facet_col='split', title='Learning Curves')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"iIK9aXVpjuRd","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","imps = model.get_score(importance_type='gain').items()\n","df_imp = pd.DataFrame(imps, columns=['feature', 'importance'])\n","df_imp = df_imp.set_index('feature').sort_values('importance', ascending=False)\n","df_imp.plot(kind='bar', y='importance', title='Feature Importances - Gain')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"t49TzqNPq5pg"},"cell_type":"markdown","source":["## Prepare Prediction Data"]},{"metadata":{"id":"GXZ10jxVjuRf"},"cell_type":"markdown","source":["### Download Final Users State"]},{"metadata":{"trusted":true,"id":"3eJlJt1FjuRg","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":["# <hide-input><hide-output>\n","# not using this currently, creating dataframe from users-content\n","# table in submission notebook\n","if False:\n","    query = Q.select_user_final_state(table_id='train')\n","    prefix = bqh.export_query_gcs(query, wait=True)\n","    file_paths = bqh.get_table_gcs(prefix)\n","    df_users = (bqh.get_df_files(file_paths, dtypes=dtypes)\n","                .reset_index(drop=True).set_index('user_id'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Xok_9xlD7c0H"},"cell_type":"markdown","source":["### Download Final User-Content State"]},{"metadata":{"id":"RUVdoI5V8FD1","_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":["# <hide-input><hide-output>\n","if False:\n","    query = Q.select_user_content_final_state(table_id='train')\n","    prefix = bqh.export_query_gcs(query, wait=True)\n","    file_paths = bqh.get_table_gcs(prefix)\n","    df_users_content = (bqh.get_df_files(file_paths, dtypes=dtypes)\n","                        .sort_values(['user_id', 'content_id']))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"tWvj0FYpjuRj"},"cell_type":"markdown","source":["### Download Questions Table"]},{"metadata":{"trusted":true,"id":"mG3gRUwmjuRl","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":["# <hide-input><hide-output>\n","if False:\n","    # only 13k rows, so it downloaded directly from BigQuery\n","    df_questions = bqh.get_df_table('questions',\n","                                    max_results=None,\n","                                    dtypes=dtypes).sort_values('question_id')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"_NH5WM3VjuRn"},"cell_type":"markdown","source":["### Update Submission Dataset"]},{"metadata":{"trusted":true,"id":"x0B8sbkGjuRo","_kg_hide-input":true},"cell_type":"code","source":["# <hide-input>\n","if False:\n","    Path(KAGGLE_SUBMIT_DATASET).mkdir(exist_ok=True)\n","\n","    model.save_model(f'{KAGGLE_SUBMIT_DATASET}/model.xgb')\n","\n","    with open(f'{KAGGLE_SUBMIT_DATASET}/columns.json', 'w') as cj:\n","            json.dump(columns_train, cj)\n","    \n","    df_files = {\n","        # 'df_users.pkl': df_users,\n","        'df_users_content.pkl': df_users_content,\n","        'df_questions.pkl': df_questions,\n","    }\n","\n","    for file_path, df in df_files.items():\n","        df.to_pickle(f'{KAGGLE_SUBMIT_DATASET}/{file_path}')\n","            \n","    kaggle_id = f\"{os.getenv('KAGGLE_USERNAME')}/{KAGGLE_SUBMIT_DATASET}\"\n","    \n","    metadata = {\n","        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n","        \"id\": kaggle_id,\n","        \"title\": KAGGLE_SUBMIT_DATASET\n","           }\n","\n","    with open(f'{KAGGLE_SUBMIT_DATASET}/dataset-metadata.json', 'w') as f:\n","        json.dump(metadata, f)\n","            \n","    if kaggle_api.dataset_status(kaggle_id):\n","        kaggle_api.dataset_create_version(KAGGLE_SUBMIT_DATASET,\n","                                          version_notes='update dataset',\n","                                          delete_old_versions=True,\n","                                          dir_mode='tar',\n","                                          quiet=True\n","                                         )\n","    else:\n","        kaggle_api.dataset_create_new(KAGGLE_SUBMIT_DATASET,\n","                                      dir_mode='tar', quiet=True)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"HN5k3_N-juRp"},"cell_type":"markdown","source":["## Submit From Kernel"]},{"metadata":{"id":"BGXlGXmjjuRq"},"cell_type":"markdown","source":["* Go to [RIIID Submit](https://www.kaggle.com/calebeverett/riiid-submit), fork and update to reference your dataset."]},{"metadata":{"id":"d-RK0etUwdw3"},"cell_type":"markdown","source":["## Push Kernel to Kaggle"]},{"metadata":{"id":"XoJcDapFwhCT","_kg_hide-input":true,"trusted":false},"cell_type":"code","source":["# <hide-input>\n","if NOT_KAGGLE:\n","    if True:\n","        \n","        code_file = 'riiid-2020.ipynb'\n","        with open(DRIVE/REPO/code_file, 'r') as nb:\n","            nb_json = json.load(nb)       \n","        \n","        for i, cell in enumerate(nb_json['cells']):\n","            if cell['cell_type'] == 'code':\n","                \n","                # update show/hide code cells\n","                for h in ['input', 'output']:\n","                    if cell['source'][0].find(f'<hide-{h}') > 1:\n","                        nb_json['cells'][i]['metadata'].update({f'_kg_hide-{h}': True})\n","                    else:\n","                        nb_json['cells'][i]['metadata'].pop(f'_kg_hide-{h}', None)\n","\n","                # add modules as cells\n","                if len(cell['source']) == 1:\n","                    groups = re.search(r'(?<=\\<include-)(.*?)(?=\\>)', cell['source'][0])\n","                    \n","                    if groups:\n","                        with open(DRIVE/REPO/groups.group(0), 'r') as m:\n","                            nb_json['cells'][i]['source'] = m.readlines() + nb_json['cells'][i]['source']    \n","\n","\n","        if Path(code_file).exists():\n","            Path(code_file).unlink()\n","        \n","        with open(f'{code_file}', 'w') as f:\n","            json.dump(nb_json, f)\n","\n","        data = {'id': 'calebeverett/riiid-bigquery-xgboost-end-to-end',\n","                        'title': 'RIIID: BigQuery-XGBoost End-to-End',\n","                        'code_file': code_file,\n","                        'language': 'python',\n","                        'kernel_type': 'notebook',\n","                        'is_private': 'false',\n","                        'enable_gpu': 'true',\n","                        'enable_internet': 'true',\n","                        'dataset_sources': [],\n","                        'competition_sources': ['riiid-test-answer-prediction'],\n","                        'kernel_sources': []}\n","        \n","        with open('kernel-metadata.json', 'w') as f:\n","            json.dump(data, f)\n","\n","        kaggle_api.kernels_push('.')"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python_defaultSpec_1607100323759","display_name":"Python 3.7.9 64-bit ('riid_kaggle': conda)","language":"python"}},"nbformat":4,"nbformat_minor":4}